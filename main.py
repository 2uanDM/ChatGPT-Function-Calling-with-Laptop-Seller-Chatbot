import json
import traceback
import streamlit as st
import time
import sys
import os
import sqlite3
from datetime import datetime
from openai import OpenAI

from src.utils.gpt_trigger_function import build_filter_query, print_haha
from src.utils.csv_builder import create_temp_csv_file
from src.utils.chat_on_demand import chat_on_demand
from src.utils.load_prompts import load_prompts

if 'current_query' not in st.session_state:
    st.session_state['current_query'] = None

if 'remain_laptops' not in st.session_state:
    st.session_state['remain_laptops'] = None

if 'current_laptops' not in st.session_state:
    st.session_state['current_laptops'] = None


def current_context_calculator() -> int:
    text = ''
    for msg in st.session_state.messages:
        text += msg['content']

    tokens: int = (len(text) % 16000) // 4

    return tokens


def my_exception_hook(exctype, value, tb):
    formatted_time = datetime.now().strftime("%d-%m-%Y %H:%M:%S")

    os.makedirs("logs", exist_ok=True)

    with open("logs/error.log", "a", encoding='utf-8') as f:
        f.write(f"==>> Time: {formatted_time}\n")
        f.write(f"==>> Type: {exctype}\n")
        f.write(f"==>> Value: {value}\n")
        f.write("==>> Traceback:\n")
        traceback.print_tb(tb, file=f)
        f.write("\n")

    print(f"==>> Type: {type}")
    print(f"==>> Value: {value}")
    traceback.print_tb(tb)


def _type_writer(text: str, speed: int = 100) -> None:
    if text is None or text == '':
        return
    tokens = text
    container = st.empty()
    for index in range(len(tokens) + 1):
        curr_full_text = tokens[:index]
        container.markdown(curr_full_text)
        time.sleep(1 / speed)


def release_context_token() -> None:
    """
        This function will keep the last answer of the chatbot and user to release the context token.
    """
    if st.session_state.remain_laptops:

        # Continue get max 5 laptops
        header = st.session_state.remain_laptops.split('\n')[0]
        send_content = '\n'.join(st.session_state.remain_laptops.split('\n')[1:5])
        remain_content = '\n'.join(st.session_state.remain_laptops.split('\n')[5:])

        st.session_state.remain_laptops = header + '\n' + remain_content

        # Blank the message
        st.session_state.messages = []

        # Load system guide
        st.session_state.messages.append(
            {"role": "system", "content": load_prompts('system_guide_preview_query')}
        )
        st.session_state.conversation.append(
            {"role": "system", "content": load_prompts('system_guide_preview_query')}
        )

        st.session_state.messages.append(
            {"role": "system", "content": f'ƒê√¢y l√† k·∫øt qu·∫£ m·ªõi nh·∫•t g·ªìm nh·ªØng laptop ph√π h·ª£p v·ªõi ti√™u ch√≠ ng∆∞·ªùi d√πng ch·ªçn ra. H√£y ghi nh·ªõ n√≥ ƒë·ªÉ t∆∞ v·∫•n th·∫≠t nhi·ªát t√¨nh cho ng∆∞·ªùi d√πng nh√©\n {header}\n{send_content}'}
        )
        st.session_state.conversation.append(
            {"role": "system", "content": f'ƒê√¢y l√† k·∫øt qu·∫£ m·ªõi nh·∫•t g·ªìm nh·ªØng laptop ph√π h·ª£p v·ªõi ti√™u ch√≠ ng∆∞·ªùi d√πng ch·ªçn ra. H√£y ghi nh·ªõ n√≥ ƒë·ªÉ t∆∞ v·∫•n th·∫≠t nhi·ªát t√¨nh cho ng∆∞·ªùi d√πng nh√©\n {header}\n{send_content}'}
        )

    else:
        header = ''
        send_content = ''
        remain_content = ''

    # Keep last answer
    last_answer = st.session_state.messages[-1]

    # Add last answer
    st.session_state.messages.append(
        {"role": "system", "content": f'ƒê√¢y l√† c√¢u n√≥i/ c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng. H√£y ti·∫øp t·ª•c tr·∫£ l·ªùi n√≥: {last_answer}'}
    )
    st.session_state.conversation.append(
        {"role": "system", "content": f'ƒê√¢y l√† c√¢u n√≥i/ c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng. H√£y ti·∫øp t·ª•c tr·∫£ l·ªùi n√≥: {last_answer}'}
    )

    with st.spinner('Thinking ...'):
        message = chat_on_demand(
            messages=st.session_state.messages
        )

    _type_writer(message, speed=100)


def store_user_requirement(content: str) -> None:
    st.session_state['user_requirement'] = content


def get_laptop_detail(which_one: str):
    if st.session_state.current_laptops:
        # Get the second last message
        second_last_message = st.session_state.messages[-2]

        # Get the last message
        last_message = st.session_state.messages[-1]

        # Get the current laptop
        current_laptop = st.session_state.current_laptops

        with st.spinner('Thinking ...'):
            response = chat_on_demand(
                messages=[
                    second_last_message,
                    last_message,
                    {'role': 'system', 'content': f'Ng∆∞·ªùi d√πng mu·ªën xem th√¥ng tin chi ti·∫øt c·ªßa m√°y ƒë√≥: {which_one}, h√£y tr·∫£ l·ªùi d·ª±a tr√™n nh·ªØng chi·∫øc laptop n√†y nh√©, nh·ªõ l√† t√¨m ki·∫øm xem ng∆∞·ªùi d√πng mu·ªën xem m√°y n√†o: {current_laptop}'},
                ]
            )

        st.session_state.messages.append(
            {'role': 'assistant', 'content': response}
        )
        st.session_state.conversation.append(
            {'role': 'assistant', 'content': response}
        )

        _type_writer(response, speed=100)


def discovery_more_laptop():
    if st.session_state.remain_laptops:
        # Get the second last message
        second_last_message = st.session_state.messages[-2]

        # Get the last message
        last_message = st.session_state.messages[-1]

        # Get the current laptop
        remain_laptops = st.session_state.remain_laptops

        with st.spinner('Thinking ...'):
            response = chat_on_demand(
                messages=[
                    second_last_message,
                    last_message,
                    {'role': 'system', 'content': f'Ng∆∞·ªùi d√πng mu·ªën xem th√™m laptop, h√£y tr·∫£ l·ªùi d·ª±a tr√™n nh·ªØng chi·∫øc laptop n√†y nh√©, nh·ªõ l√† t√¨m ki·∫øm d·ª±a tr√™n nhu c·∫ßu c·ªßa ng∆∞·ªùi d√πng: {st.session_state.user_requirement}. Laptop data is {remain_laptops}'},
                ]
            )

        st.session_state.messages.append(
            {'role': 'assistant', 'content': response}
        )
        st.session_state.conversation.append(
            {'role': 'assistant', 'content': response}
        )

        _type_writer(response, speed=100)
    else:
        st.session_state.messages.append(
            {'role': 'system', 'content': 'Hi·ªán t·∫°i kh√¥ng c√≤n laptop n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng n·ªØa r·ªìi, h√£y t√¨m ki·∫øm l·∫°i nh√©'}
        )
        st.session_state.conversation.append(
            {'role': 'system', 'content': 'Hi·ªán t·∫°i kh√¥ng c√≤n laptop n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng n·ªØa r·ªìi, h√£y t√¨m ki·∫øm l·∫°i nh√©'}
        )

        _type_writer('Hi·ªán t·∫°i kh√¥ng c√≤n laptop n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng n·ªØa r·ªìi, h√£y t√¨m ki·∫øm l·∫°i nh√©', speed=100)


def queries_db(**kwargs):
    print('==========> Running queries_db function')

    # Calculate the current context, if it is greater than 85% of the total context, then release the context
    current_token = current_context_calculator()

    if current_token >= int(16000 * 85 / 100):
        # Release the context
        release_context_token()

    # First, release the current query and remain laptops
    st.session_state.current_query = None
    st.session_state.remain_laptops = None

    st.session_state.messages.append(
        {"role": "system", "content": "Ng∆∞·ªùi d√πng ƒëang mu·ªën t√¨m ki·∫øm laptop v·ªõi c√°c th√¥ng s·ªë nh∆∞ sau"}
    )

    # Next, store the user requirement
    store_user_requirement(kwargs.get('content', ''))

    max_retry = 5
    while max_retry > 0:
        conn = sqlite3.connect('database/ttchat.db')
        try:
            query = build_filter_query(**kwargs)

            with st.spinner('Thinking ...'):
                remind_query = chat_on_demand(messages=[
                    {"role": "system", "content": f"B·∫°n nh·∫Øc l·∫°i 1 ch√∫t v·ªÅ c√°c ti√™u ch√≠ b·ªô l·ªçc m√† b·∫°n ƒë√£ l·ª±a ch·ªçn: {query} v√† gi·∫£i th√≠ch v√¨ sao b·∫°n l·∫°i ch·ªçn n√≥ cho ng∆∞·ªùi d√πng hi·ªÉu. Sau khi gi·∫£i th√≠ch xong, nh·ªõ n√≥i c√¢u: Ti·∫øp theo, m√¨nh s·∫Ω t√¨m tr√™n c∆° s·ªü d·ªØ li·ªáu t·ª´ nh·ªØng ti√™u ch√≠ n√†y"}
                ])

            st.session_state.messages.append(
                {"role": "system", "content": remind_query}
            )

            st.session_state.conversation.append(
                {"role": "system", "content": remind_query}
            )

            _type_writer(remind_query, speed=100)

            st.session_state.messages.append(
                {"role": "system", "content": f'ƒêang th·ª±c hi·ªán truy v·∫•n: {query}'}
            )
            st.session_state.conversation.append(
                {"role": "system", "content": f'ƒêang th·ª±c hi·ªán truy v·∫•n: {query}'}
            )
            print(f'==========> Query: {query}')
            query_result = conn.execute(query).fetchall()
        except Exception as e:
            print(traceback.format_exc())
            st.error('ƒê√£ x·∫£y ra l·ªói khi truy v·∫•n CSDL, ƒëang th·ª≠ l·∫°i')
            max_retry -= 1
            time.sleep(1)
        else:
            conn.close()
            break

    if max_retry == 0:
        st.error('ƒê√£ x·∫£y ra l·ªói khi truy v·∫•n CSDL, vui l√≤ng kh·ªüi ƒë·ªông l·∫°i')
        st.stop()

    # Now convert the query result to a csv file
    headers = ['id', 'product_name', 'url', 'present_price', 'old_price', 'discount', 'manufacturer', 'raw_html_path', 'laptop_type', 'cpu', 'cpu_generation', 'disk_type', 'disk_size', 'ram_gb',
               'max_ram_slot', 'screen_size', 'screen_resolution', 'screen_ratio', 'screen_refresh_rate', 'gpu_type', 'gpu_model', 'weight_kg', 'ports', 'special_features', 'release_year']

    filename = 'current_query.csv'

    csv_path = create_temp_csv_file(headers, query_result, filename)

    if not csv_path:
        st.error('ƒê√£ x·∫£y ra l·ªói khi t·∫°o file csv, vui l√≤ng th·ª≠ l·∫°i')
        st.session_state.query_result = None
    else:
        st.session_state.query_result = csv_path

    if len(query_result) == 0:
        st.session_state.conversation.append(
            {"role": "assistant",
                "content": "Ui, hi·ªán t·∫°i kh√¥ng c√≥ laptop n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n r·ªìi. B·∫°n c√≥ th·ªÉ thay ƒë·ªïi c·∫•u h√¨nh ho·∫∑c c√°c th√¥ng s·ªë 1 ch√∫t (v√≠ d·ª• nh∆∞ thay ƒë·ªïi m·ª©c gi√°, thay ƒë·ªïi dung l∆∞·ª£ng RAM, thay ƒë·ªïi dung l∆∞·ª£ng ·ªï c·ª©ng,...) ƒë·ªÉ m√¨nh t√¨m ki·∫øm l·∫°i nh√©"}
        )

        st.chat_message("assistant").write(
            "Ui, hi·ªán t·∫°i kh√¥ng c√≥ laptop n√†o ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n r·ªìi. B·∫°n c√≥ th·ªÉ thay ƒë·ªïi c·∫•u h√¨nh ho·∫∑c c√°c th√¥ng s·ªë 1 ch√∫t (v√≠ d·ª• nh∆∞ thay ƒë·ªïi m·ª©c gi√°, thay ƒë·ªïi dung l∆∞·ª£ng RAM, thay ƒë·ªïi dung l∆∞·ª£ng ·ªï c·ª©ng,...) ƒë·ªÉ m√¨nh t√¨m ki·∫øm l·∫°i nh√©")
    else:
        st.session_state.messages.append(
            {"role": "assistant",
                "content": f"ƒê√£ t√¨m th·∫•y {len(query_result)} laptop ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n, tr∆∞·ªõc ti√™n m√¨nh gi·ªõi thi·ªáu qua 5 m·∫´u laptop ph√π h·ª£p nh·∫•t nh√©"}
        )
        st.session_state.conversation.append(
            {"role": "assistant",
                "content": f"ƒê√£ t√¨m th·∫•y {len(query_result)} laptop ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n, tr∆∞·ªõc ti√™n m√¨nh gi·ªõi thi·ªáu qua 5 m·∫´u laptop ph√π h·ª£p nh·∫•t nh√©"}
        )
        st.chat_message("assistant").write(
            f"ƒê√£ t√¨m th·∫•y {len(query_result)} laptop ph√π h·ª£p v·ªõi y√™u c·∫ßu c·ªßa b·∫°n, tr∆∞·ªõc ti√™n m√¨nh gi·ªõi thi·ªáu qua 5 m·∫´u laptop ph√π h·ª£p nh·∫•t nh√©")

        # Now send the query result to the chatbot
        with open(csv_path, 'r', encoding='utf-8') as f:
            csv_content = f.read().split('\n')

        # Just send the first 5 rows ~ 5 laptops. The remaining will be store in the session state
        header = csv_content[0]
        send_content = '\n'.join(csv_content[1:5])
        remain_content = '\n'.join(csv_content[5:])

        st.session_state.remain_laptops = header + '\n' + remain_content
        st.session_state.current_laptops = header + '\n' + send_content

        st.session_state.messages.append(
            {"role": "system", "content": f'ƒê√¢y l√† k·∫øt qu·∫£ m·ªõi nh·∫•t g·ªìm nh·ªØng laptop ph√π h·ª£p v·ªõi ti√™u ch√≠ ng∆∞·ªùi d√πng ch·ªçn ra. H√£y ghi nh·ªõ n√≥ ƒë·ªÉ t∆∞ v·∫•n th·∫≠t nhi·ªát t√¨nh cho ng∆∞·ªùi d√πng nh√©\n {st.session_state.current_laptops}'}
        )

        st.session_state.messages.append(
            {"role": "system", "content": load_prompts('system_guide_preview_query')}
        )
        with st.spinner('Thinking ...'):
            message = chat_on_demand(
                messages=st.session_state.messages
            )

        st.session_state.conversation.append(
            {"role": "assistant", "content": message}
        )
        st.session_state.messages.append(
            {"role": "assistant", "content": message}
        )

        _type_writer(message, speed=100)


sys.excepthook = my_exception_hook


class ChatGUI():

    with open('config/bot.json', 'r', encoding='utf-8') as f:
        BOT_CONFIG = json.load(f)

    with open('config/function_description.json', 'r', encoding='utf-8') as f:
        FUNCTION_DESCRIPTION = json.load(f)

    date = datetime.now().strftime("%d/%m/%Y")

    def __init__(self) -> None:

        self._init_session_state()
        self._init_sidebar()
        self._init_chatui()

        # Init OpenAI Client
        self.client = OpenAI(api_key=self.BOT_CONFIG['openai_api_key'])

        # Init message event
        self._message_event()

    def trigger_function(self, status: bool = False, func_name: str = '', args: dict = {}) -> None:
        """
        Trigger the function in the backend
        For example: if status is True, and func_name is queries_db
        then run queries_db(*args)
        Args:
            func_name (str): name of function
            args (dict): 
            {
                'arg1': value1,
                'arg2': value2,
            }
        """

        if status:
            if func_name in globals() and callable(globals()[func_name]):
                if args != {}:
                    globals()[func_name](**args)
                else:
                    globals()[func_name]()

    # --------------------------- Function for init --------------------------- #

    def _init_session_state(self) -> None:
        if "messages" not in st.session_state:
            st.session_state["messages"] = [
                {"role": "system", "content": load_prompts('system_init')},
                {"role": "assistant", "content": load_prompts('assistant_init')}
            ]

        if "conversation" not in st.session_state:
            st.session_state["conversation"] = [
                {"role": "system", "content": load_prompts('system_init')},
                {"role": "assistant", "content": load_prompts('assistant_init')}
            ]

    def _init_sidebar(self) -> None:
        self.openai_api_key = st.sidebar.text_input(
            label='Nh·∫≠p OpenAI API Key c·ªßa b·∫°n:',
            value=self.BOT_CONFIG['openai_api_key'],
            key='chatbot_api_key',
            type='password')

        self.save_bot_config = st.sidebar.button(label='L∆∞u c·∫•u h√¨nh', use_container_width=True)

        if self.save_bot_config:
            self.BOT_CONFIG['openai_api_key'] = self.openai_api_key
            with open('config/bot.json', 'w', encoding='utf-8') as f:
                json.dump(self.BOT_CONFIG, f, ensure_ascii=False, indent=4)
            st.success('ƒê√£ l∆∞u c·∫•u h√¨nh th√†nh c√¥ng')

    def _init_chatui(self) -> None:
        st.title("üí¨TTChat - C√πng mua laptop nh√©")
        st.caption(f"üöÄ D·ªØ li·ªáu v·ªÅ laptop ƒë∆∞·ª£c c·∫≠p nh·∫≠t ƒë·∫øn ng√†y {self.date}")

        if not self.openai_api_key:
            st.info("Vui l√≤ng nh·∫≠p OpenAI API Key ƒë·ªÉ ti·∫øp t·ª•c")
            st.stop()

        # Load message history of session into chat message
        print(st.session_state.conversation)
        for msg in st.session_state.conversation:
            if msg['role'] != 'system':
                st.chat_message(msg["role"]).write(msg["content"])

    def _refresh_role(self) -> None:
        current_token = current_context_calculator()

        print(f'Current tokens in context: {current_token}')

        if current_token >= int(16000 * 85 / 100):
            # Release the context
            release_context_token()

            st.session_state.messages.append(
                {'role': 'system', 'content': load_prompts('system_init')}
            )

            print('==========> Mission refreshed for chatbot to remember its role !')

    def _get_response(self) -> str:
        max_retry = 5

        while max_retry > 0:
            try:
                self._refresh_role()
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo-1106",
                    messages=st.session_state.messages,
                    timeout=30,
                    max_tokens=4096,
                    temperature=1,
                    function_call='auto',
                    functions=self.FUNCTION_DESCRIPTION
                )

                return response
            except Exception as e:
                print(traceback.format_exc())
                st.error('ƒê√£ x·∫£y ra l·ªói khi g·ª≠i request ƒë·∫øn OpenAI, ƒëang th·ª≠ l·∫°i')
                max_retry -= 1
                time.sleep(2)

        if max_retry == 0:
            st.error('ƒê√£ x·∫£y ra l·ªói khi g·ª≠i request ƒë·∫øn OpenAI, vui l√≤ng kh·ªüi ƒë·ªông l·∫°i')
            st.stop()

    def _message_event(self) -> None:
        if prompt := st.chat_input():
            if st.session_state.current_query:
                with open(st.session_state.current_query, 'r', encoding='utf-8') as f:
                    query = f.read()
                st.session_state.messages.append(
                    {"role": "system", "content": load_prompts('system_recal_current_query') + '\n' + query}
                )

            st.session_state.messages.append({"role": "user", "content": prompt})
            st.session_state.conversation.append({"role": "user", "content": prompt})
            st.chat_message("user").write(prompt)

            with st.spinner("Thinking..."):
                response = self._get_response()
                answer = response.choices[0].message

            if answer.function_call:
                _type_writer(f'ƒêang th·ª±c hi·ªán h√†m: {answer.function_call.name}')
                self.trigger_function(
                    status=True,
                    func_name=answer.function_call.name,
                    args=json.loads(answer.function_call.arguments)
                )
            else:
                _type_writer(answer.content)
                st.session_state.messages.append(
                    {"role": "assistant", "content": response.choices[0].message.content})
                st.session_state.conversation.append(
                    {"role": "assistant", "content": response.choices[0].message.content})

            # Rerun to show the assistant's icon
            # st.rerun()

    # --------------------------- Function for trigger event --------------------------- #


if __name__ == '__main__':
    chat = ChatGUI()
